# =============================================================================
# TRM Platform - Cost Optimization Policies
# KEDA for event-driven scaling, Spot instances, and resource optimization
# =============================================================================

# -----------------------------------------------------------------------------
# 1. KEDA ScaledObject - Event-driven autoscaling
# Scales based on custom metrics (queue depth, HTTP requests, etc.)
# -----------------------------------------------------------------------------
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: trm-app-scaler
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: scaler
spec:
  scaleTargetRef:
    name: trm-app
    kind: Deployment
    apiVersion: apps/v1
  minReplicaCount: 2
  maxReplicaCount: 50
  cooldownPeriod: 300
  pollingInterval: 15
  advanced:
    restoreToOriginalReplicaCount: false
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 10
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
  triggers:
    # Scale based on HTTP request rate
    - type: metrics-api
      metadata:
        targetValue: "1000"
        url: "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=rate(http_requests_total{service='trm-app'}[2m])"
        valueLocation: "data.result.0.value.1"
    
    # Scale based on queue depth (Bull queue)
    - type: redis
      metadata:
        address: redis.trm.svc.cluster.local:6379
        passwordFromEnv: REDIS_PASSWORD
        listName: "bull:email-queue:wait"
        listLength: "50"
        activationListLength: "10"
    
    # Scale based on CPU utilization
    - type: cpu
      metadata:
        type: Utilization
        value: "70"
    
    # Scale based on memory utilization
    - type: memory
      metadata:
        type: Utilization
        value: "80"

---
# -----------------------------------------------------------------------------
# 2. KEDA ScaledObject for Workers
# -----------------------------------------------------------------------------
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: trm-worker-scaler
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: worker-scaler
spec:
  scaleTargetRef:
    name: trm-worker
    kind: Deployment
    apiVersion: apps/v1
  minReplicaCount: 1
  maxReplicaCount: 20
  cooldownPeriod: 60
  pollingInterval: 10
  triggers:
    # Scale based on job queue depth
    - type: redis
      metadata:
        address: redis.trm.svc.cluster.local:6379
        passwordFromEnv: REDIS_PASSWORD
        listName: "bull:job-queue:wait"
        listLength: "20"
        activationListLength: "5"

---
# -----------------------------------------------------------------------------
# 3. CronJob Scaler - Scale up during business hours
# -----------------------------------------------------------------------------
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: trm-business-hours-scaler
  namespace: trm
spec:
  scaleTargetRef:
    name: trm-app
    kind: Deployment
    apiVersion: apps/v1
  minReplicaCount: 3
  maxReplicaCount: 50
  triggers:
    # Scale up during business hours (Myanmar time UTC+6:30)
    - type: cron
      metadata:
        timezone: Asia/Yangon
        start: 0 8 * * 1-5    # 8 AM Monday-Friday
        end: 0 18 * * 1-5     # 6 PM Monday-Friday
        desiredReplicas: "5"

---
# -----------------------------------------------------------------------------
# 4. Pod Disruption Budget - Ensure availability during node maintenance
# -----------------------------------------------------------------------------
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: trm-app-pdb
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: trm
      app.kubernetes.io/component: app

---
# -----------------------------------------------------------------------------
# 5. Resource Quotas - Prevent resource exhaustion
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: trm-quota
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: quota
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    pods: "100"
    services: "20"
    persistentvolumeclaims: "10"

---
# -----------------------------------------------------------------------------
# 6. LimitRange - Set default resource limits
# -----------------------------------------------------------------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: trm-limits
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: limits
spec:
  limits:
    - default:
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:
        cpu: "100m"
        memory: "128Mi"
      max:
        cpu: "4"
        memory: "8Gi"
      min:
        cpu: "50m"
        memory: "64Mi"
      type: Container

---
# -----------------------------------------------------------------------------
# 7. Vertical Pod Autoscaler - Right-size containers
# -----------------------------------------------------------------------------
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: trm-app-vpa
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trm-app
  updatePolicy:
    updateMode: "Auto"  # Can be "Off", "Initial", "Recreate", or "Auto"
    minReplicas: 2
  resourcePolicy:
    containerPolicies:
      - containerName: 'trm-app'
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: 2
          memory: 4Gi
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits

---
# -----------------------------------------------------------------------------
# 8. Node Affinity for Spot Instances
# -----------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trm-app-spot
  namespace: trm
  labels:
    app.kubernetes.io/name: trm
    app.kubernetes.io/component: app
    workload: spot
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: trm
      workload: spot
  template:
    metadata:
      labels:
        app.kubernetes.io/name: trm
        workload: spot
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/capacity-type
                    operator: In
                    values:
                      - spot
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - trm
                topologyKey: kubernetes.io/hostname
      tolerations:
        - key: spot
          operator: Equal
          value: "true"
          effect: NoSchedule
      containers:
        - name: trm-app
          image: ghcr.io/trm/trm-app:latest
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 1000m
              memory: 1Gi

---
# -----------------------------------------------------------------------------
# 9. Cluster Autoscaler Configuration
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
data:
  priorities: |-
    10:
      - .*spot.*
    20:
      - .*on-demand.*

---
# -----------------------------------------------------------------------------
# 10. AWS Node Termination Handler for Spot Instances
# -----------------------------------------------------------------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: aws-node-termination-handler
  namespace: kube-system
  labels:
    app.kubernetes.io/name: aws-node-termination-handler
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: aws-node-termination-handler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: aws-node-termination-handler
    spec:
      serviceAccountName: aws-node-termination-handler
      containers:
        - name: node-termination-handler
          image: public.ecr.aws/aws-ec2/aws-node-termination-handler:v1.20.0
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 100m
              memory: 128Mi
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ENABLE_SPOT_INTERRUPTION_DRAINING
              value: "true"
            - name: ENABLE_SCHEDULED_EVENT_DRAINING
              value: "true"
            - name: ENABLE_REBALANCE_MONITORING
              value: "true"
            - name: ENABLE_REBALANCE_DRAINING
              value: "true"
            - name: CHECK_ASG_TAG_BEFORE_DRAINING
              value: "true"
            - name: MANAGED_ASG_TAG
              value: "aws-node-termination-handler/managed"

---
# -----------------------------------------------------------------------------
# Installation Instructions:
# -----------------------------------------------------------------------------
# 1. Install KEDA:
#    helm repo add kedacore https://kedacore.github.io/charts
#    helm repo update
#    helm install keda kedacore/keda --namespace keda --create-namespace
#
# 2. Install Vertical Pod Autoscaler:
#    kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.14.0/vpa-v1-crd-gen.yaml
#    kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.14.0/vpa-rbac.yaml
#    kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.14.0/vpa-updater-deployment.yaml
#
# 3. Install AWS Node Termination Handler:
#    helm repo add eks https://aws.github.io/eks-charts
#    helm install aws-node-termination-handler eks/aws-node-termination-handler \
#      --namespace kube-system \
#      --set enableSpotInterruptionDraining=true \
#      --set enableRebalanceMonitoring=true
#
# 4. Apply cost optimization policies:
#    kubectl apply -f k8s/cost-optimization.yaml
#
# 5. Verify KEDA is working:
#    kubectl get scaledobjects -n trm
#    kubectl describe scaledobject trm-app-scaler -n trm
# -----------------------------------------------------------------------------
